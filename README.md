# IngestÃ£o de Dados Transacionais em Batch via Airflow

Este projeto realiza a ingestÃ£o de dados transacionais particionados por dia com o intuito de simular uma arquitetura em camadas (medalhÃ£o): **source â†’ bronze â†’ silver**. 

Todo o pipeline Ã© executado localmente, orquestrado por uma instÃ¢ncia do **Apache Airflow**, e utilizando **Apache Spark** com suporte ao **Delta Lake**.

>>OBS: foi escolhido PySpark por questÃµes didÃ¡ticas. Em um contexto real, a massa de dados precisaria ser maior para justificar a necessidade da tecnologia. Mas, para utilizaÃ§Ã£o junto ao Delta, o PySpark foi a escolha mais simples.

---

## ğŸ—‚ Estrutura do Projeto

```
project-root/
â”‚
â”œâ”€â”€ mock_data/
â”‚   â””â”€â”€ MOCK_DATA.csv              # Base de dados completa usada como origem
â”‚
â”œâ”€â”€ datalake/                      # Pasta externa contendo os dados organizados por camadas. VocÃª precisa criar essa pasta e apontar na variÃ¡vel "DATALAKE_PATH" dentro dos arquivos
â”‚   â”œâ”€â”€ source/
â”‚   â”‚   â””â”€â”€ transaction_system/    # Arquivos CSV diÃ¡rios de entrada
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ transaction_data/      # Dados no formato Delta Lake (raw zone)
â”‚   â””â”€â”€ silver/
â”‚       â””â”€â”€ transaction_data/      # Dados tratados e deduplicados no formato Delta Lake
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ingest_transaction_data.py # DAG do Airflow responsÃ¡vel pela ingestÃ£o diÃ¡ria. VocÃª pode utilizar o comando 'cp' para copiar essa DAG para sua pasta de DAGs do Airflow.
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ mock_data_spliter.py       # Divide o MOCK_DATA.csv em 10 partes com datas diferentes
â”‚   â”œâ”€â”€ ingest_source_to_bronze.py # Realiza ingestÃ£o dos arquivos CSV para camada bronze
â”‚   â””â”€â”€ ingest_bronze_to_silver.py # Atualiza a camada silver com dados incrementais da bronze
â”‚
â”œâ”€â”€ requirements.txt               # Bibliotecas necessÃ¡rias para execuÃ§Ã£o
â””â”€â”€ README.md                      # Este arquivo
```

> âš ï¸ Certifique-se de criar manualmente a pasta `datalake/` na raiz do projeto antes da execuÃ§Ã£o e apontar o caminho absoluto nas respectivas variÃ¡veis.

---

## âš™ï¸ Como Executar

### 1. Instale as dependÃªncias
```bash
pip install -r requirements.txt
```

### 2. Gere os arquivos simulados
```bash
python scripts/mock_data_spliter.py
```

### 3. Inicie o Airflow standalone
```bash
airflow standalone
```

### 4. Ative a DAG no painel do Airflow
Acesse o Airflow em [http://localhost:8080](http://localhost:8080) e ative a DAG `INGEST_TRANSACTION_DATA`.

> A DAG estÃ¡ agendada para rodar todos os dias Ã s 03:00 (`0 3 * * *`), com suporte a **catchup** de dias anteriores. A simulaÃ§Ã£o inclui falhas nos dias em que nÃ£o existem arquivos.

---

---

## ğŸ“¦ Tecnologias Utilizadas

- **Apache Spark** (Standalone)
- **Delta Lake**
- **Apache Airflow** (Standalone)
- **Python 3.10+**
- **Pandas**

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a **MIT** e Ã© livre para reproduÃ§Ã£o e customizaÃ§Ã£o.

---

## ğŸ“¬ Contato

- [LinkedIn](https://www.linkedin.com/in/marco-caja)  
- [Instagram](https://www.instagram.com/omarcocaja)

---

